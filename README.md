#  Pipeline ETL : Scraping, Enrichissement et Stockage Cloud

##  Pr√©sentation du Projet
Ce projet est un pipeline de donn√©es complet (ETL) r√©alis√© dans le cadre de l'examen ECF. Il automatise la collecte de donn√©es web, leur nettoyage, leur enrichissement via API, et leur stockage dans une architecture hybride (SQL et Stockage Objet).

## Architecture Technique
Le projet repose sur une infrastructure conteneuris√©e via **Docker** :
- **PostgreSQL** : Stockage des donn√©es structur√©es (Livres, Citations, Librairies).
- **MinIO** : Stockage objet (S3) pour les fichiers JSON bruts et les images.
- **pgAdmin** : Interface d'administration pour la base de donn√©es.

## Flux de donn√©es (Data Pipeline)
1. **Extraction (Scrapy)** : 
   - Scraping de `books.toscrape.com` (images incluses).
   - Scraping de `quotes.toscrape.com`.
   - Pipeline de sauvegarde imm√©diate en JSON (format brut).
2. **Transformation & Enrichissement (Pandas & API)** :
   - Nettoyage des prix et des types de donn√©es.
   - Appel √† l'**API Adresse (Gouv.fr)** pour g√©olocaliser les librairies partenaires (ajout de Latitude/Longitude).
3. **Chargement (PostgreSQL & Boto3)** :
   - Insertion s√©curis√©e dans Postgres (gestion des transactions et rollbacks).
   - Synchronisation finale de tout le r√©pertoire `data/` vers le bucket MinIO.


### Comment lancer le projet ?
1. **Lancer les services** (Base de donn√©es et Cloud) :
   `docker-compose up -d`
2. **Installer les outils** :
   `pip install -r requirements.txt`
3. **D√©marrer le pipeline** :
   `python main.py`

### üõ†Ô∏è Ce que fait le projet :
1. **R√©colte** : Des robots (Scrapy) vont chercher des livres et des citations sur le web.
2. **Enrichit** : On ajoute les coordonn√©es GPS des librairies via une API m√©t√©o/adresse.
3. **Nettoie** : On transforme les prix et les notes en vrais chiffres avec Pandas.
4. **Stocke** : Tout est rang√© dans une base SQL (Postgres) et les images vont dans le Cloud (MinIO).